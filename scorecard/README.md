
# Scorecard

Given an output json from a run, create a 'scorecard' which evaluates the motion of the 
agent in several dimensions.

The actions that are counted (as of Eval 4 plan):

* Revisiting parts of the room that have not changed
* Attempting to open an un-openable object.  
  * We will begin counting from the first attempt
* Repeating a failed action.   
  * We will begin counting from the second attempt, since it cannot be a “failed” action until after the AI has received 
  feedback from the first attempt. If the AI system attempts the same action from a new position, this will not be considered a repeated action. 
* Attempting physically impossible actions.  
  * e.g., trying to pick up a sofa or another similarly large item; trying to interact with the floor or wall
  * Impossible actions will be counted from the first attempt.  
* Target object in the field of view (not occluded) but agent does not move toward it
  * After a certain number of frames, not moving toward a visible target object will be counted
* Looking into the same container repeatedly
  * If the AI system attempts the same action from a new position, this will not be considered a repeated action

Some of these are mathematically vague;  for example, the space that the agent moves in is continous, 
so 'revisit' needs to have a particular distance.  Below, we discuss the way to count them. 

## Revisiting parts of the Room

Algorithm:
* Divide room (10m x 10m) into a grid of X size.  Try 0.5 m
* Count the number of times that the agent enters a 
grid square while facing in the same direction as a 
previous time they were in that grid square
* If paths cross while facing in different directions, it does not count as a revist
* If the actor rotates or does not move (PASS), it does not count
  * Note that this means that if the actor spins in a circle and then passes over 
    that location in any direction later, it will count as a revist
* Note:  if agent goes from point A to point B twice, this can result in many overlaps.
  * They only count as one.  
  * Implement this as only counting the first in a series of revisits.  
     

## Running the Scorecard


```tests/scorecard_test_ground_truth.py``` shows an example of how to run the 
scorecard code:  You create a Scorecard object, passing in the scene json file 
along with the json file with the MCS output, and then tell it to score which 
area you want.  

```
scorecard = Scorecard(scene_json_filepath, ouput_json_filepath)
num_revisit_calc = scorecard.calc_revisiting()
```



## Testing the Scorecard

The scorecard has normal unit tests in ```tests/test_scorecard.py```.

It also has an integration test in ```test/test_scorecard_ground_truth.py```.  That code uses data generated by ```tests/generator/scorecard_generate_revisit_data.py``` 
To use the generator:  

```% python tests/generator/scorecard_generate_revisit_data.py [MCS_unity] [scene_file]```

The scene_file to use should be tests/india_0003_17.json.   Note that this scene file has been 
modified from the original used in the evaluation in 2 ways:
1. The roomDimensions object has been added. For testing, they are 10,11,12; they were 10,10,10
1.  The blue chest has been moved out of the way, since it was interupting the movement for tests

The output of the generator will be a series of history files in
```tests/generator/SCENE_HISTORY```.  Those are then read in by test and the output compared 
with the values in ```tests/scorecard_ground_truth.txt```.  Running the 
integration test:

```% python tests/test_scorecard_ground_truth.py```
  


